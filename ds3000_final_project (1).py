"""DS3000_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZOVBLWdHjGuMsQaTgQTxdA-5DZBDnt-2
"""


import os
import cv2
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.svm import SVC
import xgboost as xgb
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

"""Preprocessing and label and image extraction"""

# Set folder paths
train_images_path = '/content/Train_Images/'
train_labels_path = '/content/Train_Labels/'
valid_images_path = '/content/Valid_Images/'
valid_labels_path = '/content/Valid_Labels/'
test_images_path = '/content/Test_Images/'
test_labels_path = '/content/Test_Labels/'

# Function to load labels from a directory
def load_labels(label_path):
    labels = []
    #yolo format, only take first number of first line in each txt file (this number represents the class)
    for label_file in sorted(os.listdir(label_path)):  # Iterate through all label files
        # Skip unwanted system files
        if label_file.startswith('.') or os.path.isdir(os.path.join(label_path, label_file)):
            continue

        file_path = os.path.join(label_path, label_file)
        with open(file_path, 'r') as f:
            first_line = f.readline().strip()  # Read the first line of the file
            if first_line:  # Ensure the file is not empty
                class_id = int(first_line.split()[0])  # Extract the first number (class ID)
                labels.append(class_id)  # Add only the class ID to the labels list
    return labels

# Load images and preprocess
def load_images(image_path, target_size=(256, 256)):
    images = []
    for file_name in sorted(os.listdir(image_path)):
        img = cv2.imread(os.path.join(image_path, file_name))
        img = cv2.resize(img, target_size)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0  # Normalize
        images.append(img)
    return np.array(images)

# Load images and labels
X_train = load_images('Train_Images')
y_train = load_labels('Train_Labels')  # Labels now contain class IDs and bounding boxes

X_val = load_images('Valid_Images')
y_val = load_labels('Valid_Labels')

X_test = load_images('Test_Images')
y_test = load_labels('Test_Labels')

"""Feature extraction/histogram plotting"""

# Function to extract color histogram features
def extract_color_histogram(image, bins=(8, 8, 8)):
    # Convert the image to HSV color space
    hsv = cv2.cvtColor((image * 255).astype('uint8'), cv2.COLOR_RGB2HSV)
    # Compute the histogram and normalize
    hist = cv2.calcHist([hsv], [0, 1, 2], None, bins, [0, 180, 0, 256, 0, 256])
    hist = cv2.normalize(hist, hist).flatten()
    return hist

  # Extract features for all datasets
X_train_features = np.array([extract_color_histogram(image) for image in X_train])
X_val_features = np.array([extract_color_histogram(image) for image in X_val])
X_test_features = np.array([extract_color_histogram(image) for image in X_test])

"""Random forest model training"""

# Initialize Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=40)
rf_model.fit(X_train_features, y_train)

# Evaluate on the validation set
y_val_pred_rf = rf_model.predict(X_val_features)
print("Random Forest Classification Report:\n", classification_report(y_val, y_val_pred_rf))

"""SVM (support vector machine)"""

# Initialize SVM
svm_model = SVC(kernel='linear', random_state=40)
svm_model.fit(X_train_features, y_train)

# Evaluate on the validation set
y_val_pred_svm = svm_model.predict(X_val_features)
print("SVM Classification Report:\n", classification_report(y_val, y_val_pred_svm))

"""XGBoost grad boosting"""

# Convert datasets into DMatrix format (optimized for XGBoost)
dtrain = xgb.DMatrix(X_train_features, label=y_train)
dval = xgb.DMatrix(X_val_features, label=y_val)

# Define XGBoost parameters
params = {
    'objective': 'multi:softmax',  # Multi-class classification
    'num_class': len(np.unique(y_train)),  # Number of classes
    'eval_metric': 'merror',
    'max_depth': 7,
    'eta': 0.2
}

# Train the XGBoost model
xgb_model = xgb.train(params, dtrain, num_boost_round=100)

# Evaluate on the validation set
y_val_pred_xgb = xgb_model.predict(dval)
print("XGBoost Classification Report:\n", classification_report(y_val, y_val_pred_xgb))

"""Evaluate against test data"""

# Predict on the test set
y_test_pred_rf = rf_model.predict(X_test_features)
print("Random Forest Test Set Classification Report:\n", classification_report(y_test, y_test_pred_rf))

y_test_pred_svm = svm_model.predict(X_test_features)
print("SVM Test Set Classification Report:\n", classification_report(y_test, y_test_pred_svm))

dtest = xgb.DMatrix(X_test_features)
y_test_pred_xgb = xgb_model.predict(dtest)
y_test_pred_xgb = y_test_pred_xgb.astype(int)
print("XGBoost Test Set Classification Report:\n", classification_report(y_test, y_test_pred_xgb))

"""Confusion matrix"""

# Function to plot confusion matrix
def plot_confusion_matrix(y_true, y_pred, model_name):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))
    plt.title(f"Confusion Matrix - {model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# Example: Plot for Random Forest
plot_confusion_matrix(y_test, y_test_pred_rf, "Random Forest")
#Plot for SVM
plot_confusion_matrix(y_test, y_test_pred_svm, "SVM")
#Plot for xgb
plot_confusion_matrix(y_test, y_test_pred_xgb, "XGBoost")


"""Cross-validation time"""

# Evaluate Random Forest using 5-fold cross-validation
cv_scores_rf = cross_val_score(rf_model, X_train_features, y_train, cv=5)
print(f"Random Forest 5-fold CV accuracy: {cv_scores_rf.mean():.4f}")

# Evaluate SVM using 5-fold cross-validation
cv_scores_svm = cross_val_score(svm_model, X_train_features, y_train, cv=5)
print(f"SVM 5-fold CV accuracy: {cv_scores_svm.mean():.4f}")

# Evaluate xgb using 5-fold cross-validation
# Perform cross-validation with XGBoost's built-in method
xgb_cv_results = xgb.cv(params, dtrain, num_boost_round=100, nfold=5, metrics="merror", seed=40)
# Print average accuracy across the folds
xgb_mean_accuracy = 1 - xgb_cv_results['test-merror-mean'].iloc[-1]
print(f"XGBoost 5-fold CV accuracy: {xgb_mean_accuracy:.4f}")

"""Evaluate models"""

# Initialize a dictionary to store the models and their respective scores
model_scores = {}

#random forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=40)

# Train the Random Forest model on the training features and labels
rf_model.fit(X_train_features, y_train)

# Use the trained Random Forest model to make predictions on the test set
y_test_pred_rf = rf_model.predict(X_test_features)

# Calculate the accuracy of the Random Forest model's predictions
rf_accuracy = accuracy_score(y_test, y_test_pred_rf)

# Store the trained Random Forest model and its accuracy in the dictionary
model_scores['Random Forest'] = {'model': rf_model, 'accuracy': rf_accuracy}

# Initialize an SVM model with a linear kernel
svm_model = SVC(kernel='linear', random_state=40)

# Train the SVM model on the training features and labels
svm_model.fit(X_train_features, y_train)

# Use the trained SVM model to make predictions on the test set
y_test_pred_svm = svm_model.predict(X_test_features)

# Calculate the accuracy of the SVM model's predictions
svm_accuracy = accuracy_score(y_test, y_test_pred_svm)

# Store the trained SVM model and its accuracy in the dictionary
model_scores['SVM'] = {'model': svm_model, 'accuracy': svm_accuracy}


# Convert the training and test datasets into XGBoost's DMatrix format
dtrain = xgb.DMatrix(X_train_features, label=y_train)
dtest = xgb.DMatrix(X_test_features, label=y_test)

# Train the XGBoost model using the training data (dtrain)
# The model is trained for 100 boosting rounds
xgb_model = xgb.train(params, dtrain, num_boost_round=100)

# Use the trained XGBoost model to make predictions on the test set (dtest)
y_test_pred_xgb = xgb_model.predict(dtest)

# Calculate the accuracy of the XGBoost model's predictions
xgb_accuracy = accuracy_score(y_test, y_test_pred_xgb)

# Store the trained XGBoost model and its accuracy in the dictionary
model_scores['XGBoost'] = {'model': xgb_model, 'accuracy': xgb_accuracy}

# Output models + corresponding accuracies
print("Model scores:", model_scores)

"""Select the best model"""

# Select the model with the highest accuracy
best_model_name = max(model_scores, key=lambda x: model_scores[x]['accuracy'])
best_model = model_scores[best_model_name]['model']

print(f"The best model is: {best_model_name} with accuracy: {model_scores[best_model_name]['accuracy']:.4f}")

"""Use best model to make some motherfucking predictions"""

# Make predictions with the best model
if best_model_name == 'XGBoost':
    y_test_pred = best_model.predict(dtest)  # Use XGBoost's predict method
else:
    y_test_pred = best_model.predict(X_test_features)

# Evaluate final performance
from sklearn.metrics import classification_report
print(f"Classification Report for the Best Model ({best_model_name}):\n")
print(classification_report(y_test, y_test_pred))
